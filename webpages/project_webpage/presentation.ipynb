{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\">Learning about the Attention Mechanism and the Transformer Model</p>\n",
    "## <p style=\"text-align: center;\">Baptiste Amato, Alexis Durocher, Gabriel Hurtado, Alexandre Jouandin, Vincent Marois</p>\n",
    "### <p style=\"text-align: center;\">Georgia Tech - Spring 2019 CS 7643 Deep Learning Class Project - Prof. Zsolt Kira</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center; font-weight: bold;\">Abstract</p>\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teaser figure\n",
    "\n",
    "Here is an image of the `Transformer` architecture, drawn from [here](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"resources/transformer.png\" alt=\"The architecture of the Transformer model.\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction, Background and Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Transformer` model, published in 2017 by Vaswani et al ([cite](http://papers.nips.cc/paper/7181-attention-is-all-you-need)). has established state-of-the-art results in **Neural Machine Translation**, using an Encoder-Decoder architecture which does not present recurrence, breaking with previously established models (the same motivation can be found in other models, such as ConvS2S ([cite](https://arxiv.org/abs/1705.03122)) and ByteNet ([cite](https://arxiv.org/abs/1610.10099))), supposedly yielding major improvements in computational complexity.\n",
    "\n",
    "Additionally, it only relies on the attention mechanism ([cite](https://arxiv.org/abs/1409.0473)) to compute representations of its inputs and outputs. Intrinsically, this makes it an exciting problem to study. Moreover, several new models using its architecture have been published (Universal Transformers ([cite](https://arxiv.org/abs/1807.03819)), OpenAI's GPT-2 ([cite](https://openai.com/blog/better-language-models/)), and more recently their Sparse Transformer ([cite](https://arxiv.org/abs/1904.10509))), reinforcing its interest as a robust neural brick for complex models.\n",
    "\n",
    "### The Attention Mechanism\n",
    "\n",
    "An attention function can be described as **mapping a query and a set of key-value pairs to an output**, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a _compatibility_ function of the query with the corresponding key.\n",
    "\n",
    "Dot-product attention can be written as: $$Attention(Q, K, V) = Softmax(QK^T)V$$ The paper proposes _scaled_ dot-product attention, where the dot product is divided by the dimensionality of the keys.\n",
    "\n",
    "A sole instance of scaled dot-product attention is not sufficient to track the various dependencies between distant positions in the sequence. This is counter-acted with multi-head attention, where the attention is the concatenation of 8 parallel dot-product attention heads. This allows the model to jointly attend to information from different representation subspaces at different positions.\n",
    "\n",
    "### The `Transformer`'s architecture\n",
    "\n",
    "The `Transformer` model can be described as follows:\n",
    "- It follows an encoder-decoder architecture, well-known for sequences transduction tasks, such as neural machine translation,\n",
    "\n",
    "- Both the encoder and the decoder are a stack of *N* identical layers,\n",
    "- The Encoder layer is constituted of a self multi-head attention block (i.e. where the queries, keys and values are all equal), followed by a feed-forward block. Both blocks are augmented by residual connections.\n",
    "- The Decoder layer is constituted of a self multi-head attention block, followed a multi-head attention block linking the output of the encoder stack (i.e. the queries come from the Decoder, while the values & keys come from the Encoder), followed by a feed-forward block. All blocks are augmented by residual connections.\n",
    "- A final softmax classifier generates next-token probabilities on the output vocabulary set.\n",
    "- The model contains 65M parameters, all learnable.\n",
    "\n",
    "#### The model's hyper-parameters\n",
    "\n",
    "The model contains the following hyper-parameters:\n",
    "- *N*: The number of layers in the encoder & decoder stack (default: 6),\n",
    "- *d_model*: the overall dimensionality of the model (for the attention vectors, inputs / outputs shape etc.) Set to 512.\n",
    "- *n_head*: Number of heads in the multi-head attention blocks.\n",
    "- *dropout*: Dropout is used at several locations in the model (e.g. in the residual connections). Set to 0.1.\n",
    "\n",
    "Yet, while the paper is clearly written, several questions come to mind when reading it, and designing its implementation:\n",
    "\n",
    "- _Is the model's behavior identical during training and inference? Indeed, given that the model is not recurrent, can it perform greedy decoding (i.e. step-by-step prediction of the next word, starting from an initial token)?_\n",
    "- _What are the mechanisms of the model to replace recurrence, if any?_\n",
    "- _What is the sensibility of the model's training to hyper-parameters?_\n",
    "- _Can the results shown in the paper be reproduced, or approached, with limited time and computational resources?_\n",
    "\n",
    "\n",
    "##### The problem \n",
    "\n",
    "Therefore, the problem we are tackling in this project is centered around the issue of _reproducibility_. This question has recently been tracting more attention from the research community (for instance, a [workshop](https://sites.google.com/view/icml-reproducibility-workshop/home) at ICLR 2019 is dedicated to it), and is thus a valid research question.\n",
    "\n",
    "##### Objectives\n",
    "Our objectives were thus to:\n",
    "\n",
    "- Deeply understand the architecture of the `Transformer `model,\n",
    "- Reimplement (correctly) the `Transformer` model,\n",
    "- Reproduce its training process and have the model converging to acceptable results,\n",
    "- Answer the above questions, i.e. provide additional _food for thoughts_ on the model to the research community, from our student's perspective.\n",
    "\n",
    "\n",
    "##### Related work\n",
    "_How is it done today, and what are the limits of current practice?_\n",
    "\n",
    "Several references can be indicated here:\n",
    "\n",
    "- The original paper, from Vaswani et al., published at NIPS 2017. They implemented the model in the `tensor2tensor` framework ([cite](http://arxiv.org/abs/1803.07416), now part of `Tensorflow`), and trained it for 3.5 days on 8 GPUs. A limitation here is thus in terms of available computing resources. Realistically, we are not able to get access to similar machines, which motivates a deeper analysis of the training procedure, to understand which characteristics are necessary, and others optional.\n",
    "- [The Annotated `Transformer`](http://nlp.seas.harvard.edu/2018/04/03/attention.html) from Harvard's NLP group. This is an detailed walkthrough of the original paper, with code snippets showing how to implement the model's architecture. While this is a great resource to _understand_ the model, its limitations concerns the training process. Indeed, it was not designed to perform a hyper-parameter search, or to be deployed on GPU instances for training. Thus, this will be an additional focus for us.\n",
    "- The `Transformer` is a relatively popular model, and several repositories propose a reimplementation in diverse frameworks (a rapid search show ~150 repositories existing on GitHub).\n",
    "\n",
    "\n",
    "##### Potential impact\n",
    "_Who cares? If you are successful, what difference will it make?_\n",
    "\n",
    "While we, from a realistic point of view, do not expect to propose significant improvements on the `Transformer` architecture, we do hope to provide significants insights on the model, its training & inference behavior, as well as a basis for further investigations. For instance, we have implement an hyper-parameter search (for both the training's parameters and the model structure): we hope this will serve to the _research community_ for future models based on the `Transformer`.\n",
    "\n",
    "The code repository can be found [here](https://gitlab.com/DeepFrench/deep-learning-project)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now detail our approach.\n",
    "\n",
    "### Input\n",
    "From our initial dataset (IWSLT 2014 TED Translation), a word vocabulary needs to be generated. Following, all encountered words are tokenized, so that close words are associated to the same token (i.e. index) and rarely used words are considered as \"unknown\" (replaced by a `<unk>` token).\n",
    "\n",
    "The last step of data pre-processing is to embed the tokenized sentences: every tokenized word of the vocabulary is associated to a vector (of 512 units). Different embeddings methods exist (such as GloVe or Word2Vec); we followed the method of the paper: trainable, initially random, embeddings.\n",
    "\n",
    "In terms of flow, the model takes in a batch of tokenized sentences (as the embeddings look-up-table changes with backpropagation).\n",
    "\n",
    "\n",
    "### Output\n",
    "A prediction consists of an estimated distribution over the words of the output vocabulary. We use the *KL Divergence* loss to compute the \"distance\" between the model's predictions and the ground truth labels. Label smoothing ([cite](https://arxiv.org/abs/1701.06548)), i.e regularizing the model by penalizing over-confident output distributions, is used with an initial _smoothing_ factor of 0.1.\n",
    "\n",
    "The optimizer is an adapted version of Adam ([cite](https://arxiv.org/abs/1412.6980)), with a specific learning rate decay guided by the following: $$lrate = d_{\\text{model}}^{-0.5}\\cdot \\min({step\\_num}^{-0.5}, {step\\_num} \\cdot {warmup\\_steps}^{-1.5})$$ The *warmup_steps* factor hence controls when the learning rate starts decaying after its initial increase.\n",
    "\n",
    "Backpropagation is done after every step; we compute the validation loss at the end of each epoch.\n",
    "\n",
    "## Initial implementation and test on the copy task\n",
    "\n",
    "We started by implementing the model (using `PyTorch` v1.0), dividing each architectural blocks (`EncoderLayer`, `DecoderLayer`, `MultiHeadAttention`, `ResidualConnection` etc.) into classes, and ensured all were passing simple unit tests (i.e. verifying the shape of the output of the forward pass, the absence of `NaN` etc.).\n",
    "\n",
    "\n",
    "This took about a week and half to finish. At this stage, we decided to ensure that the flow of the overall model (i.e. a stack of 6 `EncoderLayer`, connected to a stack of 6 `DecoderLayer`, each layer having several sub-blocks) was correct. Thus, we wanted to verify if the model was able to fully converge on a simple algorithmic task: copying its inputs to outputs.). Indeed, a working implementation of such a heavy model should not encounter any issue copying inputs to outputs.\n",
    "\n",
    "We thus created a dataset generating random algorithmic sequences and trained the model on it. Doing so further helped us on 2 aspects of the training:\n",
    "\n",
    "- Adapt the code for CUDA support. This requires several steps, such as loading the model into GPU memory, converting the input tensors to CUDA types etc.\n",
    "- Debug the model. An aspect we initially struggled with was the use of masking in the model's forward pass. Masks are used for:\n",
    "    - Masking out the padding elements in the batches. Indeed, a batch is constituted of several sequences (algorithmic or tokenized sentences), each having a different length. In order to have a fixed batch shape, the shorter sequences are padded with 0s to match the length of the longest one. A boolean mask is generated alongside the batch and passed to the model in order to hide padding elements.\n",
    "    - Masking out subsequents elements of a sequence in the `Decoder`. As the model is not recurrent, the entire input sequence is passed at once in the `Encoder`, which does not cause problems (the model is thus able to learn how to relate words at different positions in the sentence). During training, the target sentence is fed to the `Decoder` (in a _teacher forcing_ approach). Nevertheless, in order to simulate _greedy decoding_ during training, but keep the advantage of the parallelism of the model, a mask is created so that elements at position `i` in a sequence cannot be related to elements at position `i+1` and above. This is done so that the performance during inference (which is obligatorily step-by-step, i.e. we iteratively feed the prediction of the model back into the input of the `Decoder`) is preserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"resources/copy_task.jpeg\" alt=\"Training and Validation loss of the model on the copy task.\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center; font-weight: bold;\">Figure 1: Training and Validation loss curves of the model on the copy task.</p>\n",
    "\n",
    "\n",
    "As expected, the model was able to quickly & fully converge on this task, which gave us confidence that the model's forward pass was (_most likely_) correct.\n",
    "\n",
    "## Training on the real dataset\n",
    "\n",
    "We then processed to implement all missing requirements for the training:\n",
    "\n",
    "- Collecting the statistics (loss, episode index etc.) to file and to TensorBoard,\n",
    "- Implement the dataset and analyze it. We explain our approach below,\n",
    "- Implement multi-GPU support (this is easily done using `PyTorch`'s `DataParallel` mechanism),\n",
    "- Parameterize the training (specify a random seed, batch size etc.)\n",
    "- Profile and try to reduce the memory usage of the model.\n",
    "\n",
    "\n",
    "Analyzing the dataset delivered some interesting insights. For specifications, we used the IWSLT 2014 TED Translation dataset, with:\n",
    "\n",
    "- 220k training samples, 1025 for validation, 1305 for test,\n",
    "- Average sentence length: 20 (train) - 21 (val) - 19 (test),\n",
    "\n",
    "Plotting the distribution of the samples with respect to the sequence length gave the following graphs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"resources/seq_length_dist.png\" alt=\"Histogram of the training set with respect to the sequence length.\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center; font-weight: bold;\">Figure 2: Histogram (normalized and cumulated) of the training set with respect to the sequence length.</p>\n",
    "\n",
    "We can notice a long tailed dependency on the sequence length, meaning that for a sequence length of 40 (for a max of 102), we get 90% of the training set. A shorter sequence length provides the benefit of a larger batch size (for the same memory use), which generally stabilizes the training.\n",
    "\n",
    "\n",
    "## Convergence issue\n",
    "_What problems did you encounter?_\n",
    "\n",
    "Having analyzed the dataset, we started by first running the model on 40% of the training set (corresponding to a sequence length of 18) for 15 epochs to observe if we could get a correct convergence with the default hyper-parameters as indicated in the paper. We expected the validation loss to plateau, if not increase, which is what we observed:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"resources/run_40%_10_epochs.png\"  alt='Histogram of the training set with respect to the sequence length.'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center; font-weight: bold;\">Figure 3: Training (orange) and validation (blue) loss evolution over 10 epochs on 40% of the training set.</p>\n",
    "\n",
    "As can be seen, the validation loss clearly plateaus, and increases at the end of training, indicating an overfitting. Several causes could be pointed out here, such as a need for better regularization or more data.\n",
    "\n",
    "Additionally, several hyper-parameters were present in the training, and we suspected that they would greatly influence the convergence of the model. We decided to perform a hyper-parameter search (using Google Cloud's [Hyperparameter Tuning](https://cloud.google.com/ml-engine/docs/tensorflow/using-hyperparameter-tuning) feature). \n",
    "\n",
    "We found a working combination of hyper-parameters and will analyze the training we obtained in the **Experiments** section.\n",
    "\n",
    "\n",
    "## Memory Use issue\n",
    "\n",
    "Once the maximum sequence length fixed, we started experiments on GPUs. We quickly ran into issues of memory, the full model (6 layers in both the `Encoder` and `Decoder`) taking up a lot of space, leaving little for the batches, thus forcing a small batch size. \n",
    "\n",
    "To solve this issue, we profiled the memory usage of the model's forward pass, to see where the bottleneck was. We present the results in the next section and, to the best of our knowledge, this represents an analysis not previously done for the `Transformer` (_Is anything new in your approach?_).\n",
    "\n",
    "To address the issue, we also:\n",
    "- pinned the generation of the datasets to CPU, and only move the batch to CUDA memory when needed,\n",
    "- Implemented multi-gpu support. This allows a larger batch (e.g. of size 4\\*128 = 512) to be split on several GPUs, each receiving a chunk of the batch (e.g. 128 samples for 4 GPUs). Yet, this only partially solves the issue, as according to our understanding and analysis, one of the GPU devices behaves as the master node and thus sees a higher memory usage as it computes gradients and backpropagates them. The other GPUs (behaving as slave nodes) are only used for computing a forward pass. This may be linked to a `PyTorch` limitation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_How did you measure success? What experiments were used? What were the results, both quantitative and qualitative? Did you succeed? Did you fail? Why?_\n",
    "\n",
    "We present here our experiments and results.\n",
    "\n",
    "## Memory Profiling\n",
    "\n",
    "We recorded the memory usage (on a Tesla M60 with 8Gb of memory) at different instants of the forward pass (before and after the encoder, when computing loss etc.) over an epoch, to observe where the memory increase happens.\n",
    "\n",
    "The following graph shows the memory use evolution over the first 4 iterations:\n",
    "\n",
    "<img src=\"resources/precise-memory-use-4-iterations.pdf\"  alt='Memory Use evolution over the first 4 iterations.'/>\n",
    "<p style=\"text-align: center; font-weight: bold;\">Figure 4: Detailed Memory Use evolution over the first 4 iterations.</p>\n",
    "\n",
    "Several remarks can be done here:\n",
    "- The first iteration shows memory increases caused by the `Encoder` and the `Decoder`, which indicates that the corresponding weight matrices are being loaded into memory.\n",
    "- The loss computation provokes a memory increase several times over the course of the first 4 iterations. Our hypothesis is that this is linked to the computations of the gradients, which takes up space. Yet, this space is not freed at each iteration, but appear to be cached, which is why the memory use continues to initially increase.\n",
    "- While it is not shown here, after the memory peak at ~7Gb, it stabilizes at 6Gb over the rest of the epoch. This indicates that `PyTorch` is most likely optimizing by caching up tensors initially, and reusing these afterwards for a higher efficiency (we may think that deallocating & reallooating memory at each iterations slows training down).\n",
    "\n",
    "We have been able to alleviate the memory bottleneck by using GPUs with more memory (12Gb, and Colab recently added Tesla T4 with 16Gb of memory)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter tuning\n",
    "\n",
    "To obtain a set of parameters which would lead to convergence during training, we decided to perform a hyper-parameter tuning on a subset of the training parameters.\n",
    "\n",
    "Here is the subset of parameters we tuned:\n",
    "- _warmup_: The _warmup_ factor hence controls when the learning rate starts decaying after its initial increase.\n",
    "- _smoothing_: the KL Divergence loss uses label smoothing ([cite](https://arxiv.org/abs/1701.06548)). This parameter thus controls the probability of the ground-truth labels.\n",
    "- *beta\\_0, beta\\_1*: the parameters of the underlying Adam ([cite](https://arxiv.org/abs/1412.6980)) optimizer.\n",
    "\n",
    "These parameters were tuned using Google's AI Platform. We initially used a validation loss summed over all batches of our validation, but this validation loss was impacted by the `smoothing` parameter that we tuned. As such, it wasn't very relevant to compare validation loss values from two different trials with different smoothing values. \n",
    "\n",
    "A measure such as BLEU would mitigate this kind of issue: BLEU is \"smoothing-agnostic\", and considers the semantic consistency between the target and predicted sequence.\n",
    "\n",
    "Nevertheless, this HyperParameter search steered our choice of values when training the final models.\n",
    "\n",
    "<img src=\"resources/hp_tuning.png\"  alt='Training & Validation loss of the final model.'/>\n",
    "<p style=\"text-align: center; font-weight: bold;\">Figure 5: A few hyper-parameter tuning trial results from Google's HyperTune.</p>\n",
    "\n",
    "We have initially rapidly observed a relation between _warmup_ and _smoothing_, so we wanted to test several values for these 2 parameters. We added *beta\\_0*, *beta\\_1* as well as we hypothesized that their values would be influenced by _warmup_. \n",
    "\n",
    "The optimal validation loss was obtained for the following combination of the above parameters: $$warmup = 9505, smoothing= 0.241, \\beta_0=0.916, \\beta_1=0.907$$\n",
    "\n",
    "\n",
    "Following, we trained a `Transformer` model from scratch using the above combination of parameters. We used a Tesla T4 with 16Gb of memory. We trained on 90% of the training set with a batch size of 128 over 25 epochs. Training took 8.5h (~21 minutes / epoch).\n",
    "\n",
    "<img src=\"resources/val_loss_final.png\"  alt='Training & Validation loss of the final model.'/>\n",
    "<p style=\"text-align: center; font-weight: bold;\">Figure 6: Training (orange) and validation (blue) loss evolution of the best model over 25 epochs on 90% of the training set.</p>\n",
    "\n",
    "This model reached a loss of 0.7574 on the validation at the end of training, with a best obtained loss of 0.6691 (at epoch 19). There thus has been a slight overfitting towards the end of the training, but this still represents a major improvement over our initial runs where the validation loss would quickly plateau. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "Do the results make sense? Why or why not? Describe what kind of visualization/analysis you performed in order to verify that your results 1) are correct and 2) explain differences in performance from what was expected (e.g. what appeared in papers). Provide specific claims about why you think your model is or is not doing better, and justify those with qualitative and quantitative experiments (not necessarily just final accuracy numbers, but statistics or other data about what the model is doing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, validation and testing workflow\n",
    "\n",
    "The Transformer model is stated as being a non-recurrent deep-learning model. While this is true during training, this statement is — from our opinion — false during inference.\n",
    "\n",
    "During training, the recurrence is actually simulated by using a subsequent mask which hides the subsequent words of the sentence. However during the inference, we used a greedy way of decoding iteratively each word of the sentence. Hence, during inference, the complete same translation function is recurrently applied to output next word prediction until the last word token. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test traduction semantic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model task is to translate French to English sentences.\n",
    "To test our model semantic performance - i.e how good was our model at translating French to English sentences - we could use the BLEU (bilingual evaluation understudy) score which helps evaluating the quality of text which has been machine-translated from one natural language to another.\n",
    "\n",
    "An other metric is simply the human evaluation. Below are several samples from the test dataset - i.e unseen by our model during training nor during hyperparameters fine-tuning. An informed reader can then judge the quality of the translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sample 1:\n",
    "\n",
    "French source: <i> C'est un soleil placé avec l'origine , car le Japon est à l'Est de la Chine . </i> <br>\n",
    "English groundtruth (target): <i> This is a sun placed with the origin , because Japan lies to the east of China . </i> <br>\n",
    "\n",
    "English traduction (output): <b> It 's a sun put in place , because Japan is in the east of China . </b>\n",
    "\n",
    "BLEU score (target/output): 0.3414"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sample 2:\n",
    "\n",
    "French source: <i> Si quelqu'un marche derrière , ça veut dire « suivre » . </i> <br>\n",
    "English groundtruth (target): <i> If someone walks behind , that is '' to follow . '' </i> <br>\n",
    "\n",
    "English traduction (output): <b> If someone walks behind it , it means '' follow . '' </b>\n",
    "\n",
    "BLEU score (target/output): 0.4682"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From sample 1, we can see that the model correctly identified the topic, the structure and the semantic of the sentence. Note that the model translated litteraly french words instead of using more appropriate and sophisti english word.\n",
    "Indeed, \"C'est\" and \"est\" have been translated in \"It's\" and \"is\" instead of the target \"This is\" and \"lies\".\n",
    "\n",
    "This might be due to the fact that direct French-English traductions such as \"est\" in \"is\" are much more frequent in our dataset. The model wasn't train enough to realize that given a \"sun\" subject, \"est\" should be translated in \"lies\" instead of \"is\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From sample 2, we can see that the model again correctly identified the topic, the structure and the semantic of the sentence. Even more impressive, it managed to provide a sentence which actually make more sense than the original target. Again, the model translated directly french words. Indeed, \"ça veut dire\" has been translated in \"it means\" instead of the target \"that is\". We believe that again, this is a matter of training occurences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall the traductions are pretty impressive, given the short amount of training epoch (24 only) and the complexity of this model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Member Identification\n",
    "\n",
    "All project members provided equal contribution in the undertanding of the Transformer model and project webpage writing.\n",
    "\n",
    "| Name | Description |\n",
    "|----|-----|\n",
    "| Baptiste Amato  | Multi-GPU Support & Cloud instances script setup, Implementation of the optimizer / loss / training loop, hyper-parameters search   |\n",
    "| Alexis Durocher  | Initial implementation of the main classes, Dataset implementation, memory profiling, attention visualization   |\n",
    "| Gabriel Hurtado  | Dataset implementation, Implementation of the optimizer / loss / training loop, final model training   |\n",
    "| Alexandre Jouandin  | Initial implementation of the main classes, Dataset implementation, Convergence on copy task, hyper-parameters search   |\n",
    "| Vincent Marois  | Implementation of the optimizer / loss / training loop, Convergence on copy task, memory profiling   |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "title": "The Attention Mechanism and the Transformer Model",
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
